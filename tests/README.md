# ðŸ§ª TESTING GUIDE - CHAMPILYTICS

## ðŸ“‹ RESUMEN RÃPIDO

**Estado actual**: 0% â†’ Target 80% coverage  
**Framework**: pytest 8.3.3  
**Estrategia**: Unit tests con mocking completo de Google Sheets

---

## ðŸš€ INSTALACIÃ“N

```powershell
# 1. Activar entorno virtual
.\venv_local\Scripts\Activate.ps1

# 2. Instalar dependencias de testing
pip install -r requirements-dev.txt

# 3. Verificar instalaciÃ³n
pytest --version
```

---

## ðŸŽ¯ EJECUTAR TESTS

### Comandos bÃ¡sicos

```powershell
# Ejecutar TODOS los tests
pytest

# Ejecutar con verbosidad (muestra cada test)
pytest -v

# Ejecutar tests de un archivo especÃ­fico
pytest tests/test_data_manager.py

# Ejecutar un test especÃ­fico
pytest tests/test_data_manager.py::test_load_data_conexion_exitosa_devuelve_dos_dataframes

# Ejecutar tests que coincidan con un patrÃ³n
pytest -k "load_data"

# Ejecutar solo tests rÃ¡pidos (excluir lentos)
pytest -m "not slow"

# Ejecutar solo tests unitarios
pytest -m unit

# Modo verbose con output de prints
pytest -v -s
```

### Comandos de cobertura

```powershell
# Ejecutar tests con reporte de cobertura
pytest --cov=utils --cov=components --cov=views

# Reporte de cobertura en HTML (abre htmlcov/index.html)
pytest --cov=utils --cov-report=html
start htmlcov/index.html

# Reporte de cobertura en terminal con lÃ­neas faltantes
pytest --cov=utils --cov-report=term-missing

# Generar XML para CI/CD (Codecov, etc.)
pytest --cov=utils --cov-report=xml
```

### Comandos avanzados

```powershell
# Ejecutar tests en paralelo (mÃ¡s rÃ¡pido)
pytest -n auto

# Parar en el primer fallo
pytest -x

# Mostrar tests mÃ¡s lentos
pytest --durations=10

# Modo watch (re-ejecuta al guardar archivos)
pytest-watch

# Debugger interactivo en fallos
pytest --pdb
```

---

## ðŸ“‚ ESTRUCTURA DE TESTS

```
tests/
â”œâ”€â”€ __init__.py                    # Marca directorio como paquete
â”œâ”€â”€ conftest.py                    # Fixtures globales (â˜… MÃS IMPORTANTE)
â”œâ”€â”€ test_data_manager.py           # Tests de utils/data_manager.py
â”œâ”€â”€ test_helpers.py                # Tests de utils/helpers.py (TODO)
â”œâ”€â”€ test_styles.py                 # Tests de components/styles.py (TODO)
â”œâ”€â”€ test_views_landing.py          # Tests de views/landing.py (TODO)
â””â”€â”€ test_views_dashboard.py       # Tests de views/dashboard.py (TODO)
```

---

## ðŸŽ“ CONCEPTOS CLAVE DE MOCKING

### Â¿QuÃ© es un Mock?

**Mock** = Objeto falso que simula el comportamiento de uno real

```python
# âŒ SIN MOCK (llama API real)
def test_sin_mock():
    df = load_data()  # â† Llama a Google Sheets (lento, requiere internet)

# âœ… CON MOCK (usa datos falsos)
def test_con_mock(mock_conectar_sheets):
    df = load_data()  # â† No llama a Google, usa fixture (rÃ¡pido, offline)
```

### Â¿CÃ³mo funciona el mocking?

```
TU CÃ“DIGO REAL:
1. conectar_sheets() â†’ Google Sheets API
2. get_all_records() â†’ [{"id": "CTA-001", ...}, ...]
3. pd.DataFrame(...) â†’ DataFrame real

CON MOCK (en tests):
1. conectar_sheets() â†’ âŒ INTERCEPTADO â†’ Devuelve mock object
2. get_all_records() â†’ âŒ INTERCEPTADO â†’ Devuelve datos de prueba
3. pd.DataFrame(...) â†’ DataFrame con datos de prueba

TU CÃ“DIGO NO SABE QUE USA MOCKS. Es transparente.
```

### Fixtures importantes en conftest.py

| Fixture | QuÃ© hace | CuÃ¡ndo usarlo |
|---------|----------|---------------|
| `mock_streamlit_secrets` | Simula st.secrets | Cuando tu funciÃ³n lee secrets.toml |
| `mock_conectar_sheets` | Reemplaza conectar_sheets() | Para load_data(), guardar_datos() |
| `sample_cuentas_df` | DataFrame de prueba (cuentas) | Para verificar estructura de datos |
| `sample_metricas_df` | DataFrame de prueba (metricas) | Para verificar estructura de datos |
| `disable_streamlit_cache` | Desactiva @st.cache_* | AutomÃ¡tico (autouse=True) |

---

## âœï¸ ESCRIBIR TU PRIMER TEST

### Estructura bÃ¡sica (AAA Pattern)

```python
import pytest
from utils.data_manager import load_data

@pytest.mark.unit
def test_load_data_devuelve_dataframes(mock_conectar_sheets):
    """TEST: load_data() devuelve dos DataFrames"""
    
    # ARRANGE (preparar)
    # Ya hecho por fixture mock_conectar_sheets
    
    # ACT (ejecutar)
    df_cuentas, df_metricas = load_data()
    
    # ASSERT (verificar)
    assert df_cuentas is not None
    assert df_metricas is not None
    assert len(df_cuentas) > 0
    assert len(df_metricas) > 0
```

### Nomenclatura de tests

```python
# Formato: test_<funciÃ³n>_<escenario>_<resultado_esperado>

def test_load_data_conexion_exitosa_devuelve_dataframes():
    pass

def test_load_data_con_error_devuelve_dataframes_vacios():
    pass

def test_guardar_datos_engagement_invalido_devuelve_false():
    pass
```

### Markers (etiquetas)

```python
@pytest.mark.unit          # Test unitario (rÃ¡pido, sin I/O)
@pytest.mark.integration   # Test de integraciÃ³n (lento, con API real)
@pytest.mark.slow          # Test lento (> 1 segundo)
@pytest.mark.skip          # Saltar este test
@pytest.mark.parametrize   # Test parametrizado (mÃºltiples casos)
```

---

## ðŸ› DEBUGGING DE TESTS

### Ver por quÃ© fallÃ³ un test

```powershell
# Modo verbose con traceback completo
pytest -v --tb=long

# Entrar en debugger interactivo en fallos
pytest --pdb

# Ver output de prints (incluso si test pasa)
pytest -s
```

### Ejecutar solo tests fallidos

```powershell
# Ejecutar solo los que fallaron la Ãºltima vez
pytest --lf

# Ejecutar fallidos primero, luego el resto
pytest --ff
```

---

## ðŸ“Š INTERPRETAR REPORTE DE COBERTURA

### Reporte en terminal

```
---------- coverage: platform win32, python 3.13.1-final-0 -----------
Name                      Stmts   Miss  Cover   Missing
-------------------------------------------------------
utils/data_manager.py       517    120    77%   89-102, 234-256
utils/helpers.py            279    200    28%   12-45, 67-89
-------------------------------------------------------
TOTAL                       796    320    60%
```

**InterpretaciÃ³n:**
- **Stmts**: LÃ­neas de cÃ³digo ejecutables
- **Miss**: LÃ­neas NO ejecutadas por tests
- **Cover**: % de cobertura
- **Missing**: NÃºmeros de lÃ­nea sin cubrir

### Reporte HTML

```powershell
pytest --cov=utils --cov-report=html
start htmlcov/index.html
```

**Beneficios:**
- Visual (lÃ­neas verdes = cubiertas, rojas = no cubiertas)
- Click en archivo para ver detalles
- Identifica ramas if/else no probadas

---

## ðŸŽ¯ ESTRATEGIA DE TESTING

### Fase 1: Unit Tests (Esta semana)
```
âœ“ tests/test_data_manager.py (HECHO)
â˜ tests/test_helpers.py
â˜ tests/test_styles.py

Target: 80% coverage en utils/
```

### Fase 2: Integration Tests (PrÃ³xima semana)
```
â˜ tests/integration/test_sheets_real_api.py
â˜ tests/integration/test_end_to_end_flow.py

Target: Flujo completo (load â†’ process â†’ save)
```

### Fase 3: E2E Tests (Siguiente mes)
```
â˜ tests/e2e/test_streamlit_ui.py
â˜ tests/e2e/test_navigation.py

Target: Probar UI de Streamlit
```

---

## ðŸš¨ TROUBLESHOOTING

### Error: "ModuleNotFoundError: No module named 'utils'"

```powershell
# SoluciÃ³n: AsegÃºrate de estar en la raÃ­z del proyecto
cd "F:\MATRIZ DE REDES\social_media_matrix"
pytest
```

### Error: "fixture 'mock_conectar_sheets' not found"

```powershell
# SoluciÃ³n: conftest.py debe estar en tests/
# Verificar:
ls tests/conftest.py
```

### Tests muy lentos

```powershell
# SoluciÃ³n: Ejecutar en paralelo
pytest -n auto

# Ver tests mÃ¡s lentos
pytest --durations=10
```

### Mock no funciona

```python
# âŒ INCORRECTO: Importar antes de mockear
from utils.data_manager import conectar_sheets
def test_algo(mock_conectar_sheets):
    # conectar_sheets ya fue importada, mock no funciona

# âœ… CORRECTO: Importar dentro del test
def test_algo(mock_conectar_sheets):
    from utils.data_manager import load_data
    # Ahora load_data() usa el mock
```

---

## ðŸ“š RECURSOS ADICIONALES

### DocumentaciÃ³n oficial
- [Pytest](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [unittest.mock](https://docs.python.org/3/library/unittest.mock.html)

### Tutoriales recomendados
- [Real Python - Pytest Guide](https://realpython.com/pytest-python-testing/)
- [Effective Python Testing With Pytest](https://realpython.com/python-testing/)

### Cheat Sheet
```python
# Assertions comunes
assert x == y                  # Igualdad
assert x != y                  # Desigualdad
assert x > y                   # Mayor que
assert x in y                  # Pertenencia
assert x is None               # Identidad
assert isinstance(x, list)     # Tipo

# Verificar excepciones
with pytest.raises(ValueError):
    funcion_que_falla()

# Verificar warnings
with pytest.warns(UserWarning):
    funcion_con_warning()

# Verificar llamadas a mocks
mock.assert_called()           # Llamado al menos 1 vez
mock.assert_called_once()      # Llamado exactamente 1 vez
mock.assert_called_with(x, y)  # Llamado con args especÃ­ficos
mock.assert_not_called()       # Nunca llamado
```

---

## ðŸŽ‰ SIGUIENTE PASO

```powershell
# 1. Instalar dependencias
pip install -r requirements-dev.txt

# 2. Ejecutar tests
pytest -v

# 3. Ver cobertura
pytest --cov=utils --cov-report=html
start htmlcov/index.html

# 4. Escribir mÃ¡s tests (objetivo: 80% coverage)
```

**Â¡Ã‰xito! Ahora tienes testing profesional configurado.** ðŸš€
